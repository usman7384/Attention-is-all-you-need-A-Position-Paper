@inproceedings{vaswani2017attention,
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  title={Attention is All You Need},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017},
  publisher={Springer, Heidelberg},
  doi={10.5555/3295222.3295349}
}

@inproceedings{dosovitskiy2020image,
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Houlsby, Neil},
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle={Conference on Learning Representations (ICLR 2021)},
  year={2021},
  publisher={Springer, Heidelberg}
}

@inproceedings{kitaev2020reformer,
  author={Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  title={Reformer: The Efficient Transformer},
  booktitle={International Conference on Learning Representations (ICLR 2020)},
  year={2020},
  publisher={Springer, Heidelberg}
}

@article{wang2020linformer,
  author={Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  title={Linformer: Self-Attention with Linear Complexity},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{shaw2018self,
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  title={Self-Attention with Relative Position Representations},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)},
  pages={464--468},
  year={2018},
  publisher={Springer, Heidelberg}
}
